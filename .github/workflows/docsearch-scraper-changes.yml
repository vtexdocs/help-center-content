name: Help Center changed files scraper

on:
  pull_request:
    branches:
      - main
    types: [closed]

jobs:
  scrape-files:
    runs-on: ubuntu-latest
    name: Scraper running
    steps:
      - uses: actions/checkout@v4
      - name: Get changed markdown files
        id: changed-files
        uses: tj-actions/changed-files@v43
        with:
          include_all_old_new_renamed_files: "true"
          files: |
            **.md
            **.mdx
          separator: ","
      - name: Printing
        run: |
          echo "All files count: $(echo "${{ steps.changed-files.outputs.all_changed_files }}" | tr ',' '\n' | wc -l)"
          echo "Added files count: $(echo "${{ steps.changed-files.outputs.added_files }}" | tr ',' '\n' | wc -l)"
          echo "Deleted files count: $(echo "${{ steps.changed-files.outputs.deleted_files }}" | tr ',' '\n' | wc -l)"
          echo "Modified files count: $(echo "${{ steps.changed-files.outputs.modified_files }}" | tr ',' '\n' | wc -l)"
          echo "Renamed files count: $(echo "${{ steps.changed-files.outputs.all_old_new_renamed_files }}" | tr ',' '\n' | wc -l)"
          
          echo "First 100 files in each category:"
          echo "Added:"
          echo "${{ steps.changed-files.outputs.added_files }}" | tr ',' '\n' | head -n 100
          echo "Modified:"
          echo "${{ steps.changed-files.outputs.modified_files }}" | tr ',' '\n' | head -n 100
          echo "Deleted:"
          echo "${{ steps.changed-files.outputs.deleted_files }}" | tr ',' '\n' | head -n 100
          echo "Renamed:"
          echo "${{ steps.changed-files.outputs.all_old_new_renamed_files }}" | tr ',' '\n' | head -n 100
      - name: Process files
        id: process-files
        run: |
          # Function to split array into chunks
          split_array() {
            local array=("$@")
            local chunk_size=$1
            shift
            local result=()
            for ((i=0; i<${#array[@]}; i+=chunk_size)); do
              result+=("${array[@]:i:chunk_size}")
            done
            echo "${result[@]}"
          }

          # Function to extract URLs from error logs
          extract_error_urls() {
            local log_file=$1
            local error_urls=()
            while IFS= read -r line; do
              if [[ $line =~ "ERROR: Http Status:(404|500) on (https?://[^[:space:]]+)" ]]; then
                error_urls+=("${BASH_REMATCH[2]}")
              fi
            done < "$log_file"
            echo "${error_urls[@]}"
          }

          # Function to process a group of files
          process_group() {
            local added=$1
            local modified=$2
            local deleted=$3
            local renamed=$4
            local attempt=1
            local max_attempts=3
            local temp_log=$(mktemp)
            local max_files=1000  # Maximum number of files to process at once

            while [ $attempt -le $max_attempts ]; do
              echo "Attempt $attempt of $max_attempts"
              
              # Split files into smaller chunks if needed
              IFS=',' read -ra ADDED <<< "$added"
              IFS=',' read -ra MODIFIED <<< "$modified"
              IFS=',' read -ra DELETED <<< "$deleted"
              IFS=',' read -ra RENAMED <<< "$renamed"

              # Process files in chunks
              for ((i=0; i<${#ADDED[@]}; i+=max_files)); do
                local chunk_added="${ADDED[@]:i:max_files}"
                local chunk_modified="${MODIFIED[@]:i:max_files}"
                local chunk_deleted="${DELETED[@]:i:max_files}"
                local chunk_renamed="${RENAMED[@]:i:max_files}"

                echo "Processing chunk $((i/max_files + 1)) of $(((${#ADDED[@]} + max_files - 1)/max_files))"
                
                if docker run --rm \
                  -v ${{ github.workspace }}:/app \
                  -e ALGOLIA_APP_ID=${{ secrets.ALGOLIA_APP_ID }} \
                  -e ALGOLIA_API_KEY=${{ secrets.ALGOLIA_WRITE_KEY }} \
                  vtexdocs/devportal-docsearch-action \
                  --files ./configs/scraper_helpcenter_files.json \
                  --added "$chunk_added" \
                  --updated "$chunk_modified" \
                  --removed "$chunk_deleted" \
                  --renamed "$chunk_renamed" 2>&1 | tee -a "$temp_log"; then
                  continue
                fi

                # Check if error is "Argument list too long" or "Maximum object size exceeded"
                if grep -q "Argument list too long\|Maximum object size exceeded" "$temp_log"; then
                  echo "Error detected, reducing chunk size..."
                  max_files=$((max_files / 2))
                  if [ $max_files -lt 10 ]; then
                    echo "Chunk size too small, aborting"
                    rm "$temp_log"
                    return 1
                  fi
                  i=0  # Restart from beginning with smaller chunks
                  continue
                fi

                echo "Different error occurred, retrying..."
                ((attempt++))
              done
            done
            rm "$temp_log"
            return 0
          }

          # Convert comma-separated strings to arrays
          IFS=',' read -ra ADDED <<< "${{ steps.changed-files.outputs.added_files }}"
          IFS=',' read -ra MODIFIED <<< "${{ steps.changed-files.outputs.modified_files }}"
          IFS=',' read -ra DELETED <<< "${{ steps.changed-files.outputs.deleted_files }}"
          IFS=',' read -ra RENAMED <<< "${{ steps.changed-files.outputs.all_old_new_renamed_files }}"

          # Create a temporary file to store all logs
          temp_log_file=$(mktemp)

          # Process all files and capture output
          process_group \
            "${{ steps.changed-files.outputs.added_files }}" \
            "${{ steps.changed-files.outputs.modified_files }}" \
            "${{ steps.changed-files.outputs.deleted_files }}" \
            "${{ steps.changed-files.outputs.all_old_new_renamed_files }}" 2>&1 | tee "$temp_log_file"

          # Extract and report error URLs
          error_urls=($(extract_error_urls "$temp_log_file"))
          if [ ${#error_urls[@]} -gt 0 ]; then
            echo "::warning::Found ${#error_urls[@]} URLs that returned 404 or 500 errors:"
            printf '%s\n' "${error_urls[@]}" | sort -u
          fi

          # Clean up
          rm "$temp_log_file"
