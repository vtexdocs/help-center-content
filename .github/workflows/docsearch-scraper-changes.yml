name: Help Center changed files scraper

on:
  pull_request:
    branches:
      - main
    types: [closed]

jobs:
  scrape-files:
    runs-on: ubuntu-latest
    name: Scraper running
    steps:
      - uses: actions/checkout@v4
      - name: Get changed markdown files
        id: changed-files
        uses: tj-actions/changed-files@v43
        with:
          include_all_old_new_renamed_files: "true"
          files: |
            **.md
            **.mdx
          separator: ","
      - name: Save file lists
        run: |
          echo "${{ steps.changed-files.outputs.added_files }}" > added_files.txt
          echo "${{ steps.changed-files.outputs.modified_files }}" > modified_files.txt
          echo "${{ steps.changed-files.outputs.deleted_files }}" > deleted_files.txt
          echo "${{ steps.changed-files.outputs.all_old_new_renamed_files }}" > renamed_files.txt
      - name: Printing
        run: |
          echo "All files count: $(wc -l < added_files.txt)"
          echo "Added files count: $(wc -l < added_files.txt)"
          echo "Deleted files count: $(wc -l < deleted_files.txt)"
          echo "Modified files count: $(wc -l < modified_files.txt)"
          echo "Renamed files count: $(wc -l < renamed_files.txt)"
          
          echo "First 100 files in each category:"
          echo "Added:"
          head -n 100 added_files.txt
          echo "Modified:"
          head -n 100 modified_files.txt
          echo "Deleted:"
          head -n 100 deleted_files.txt
          echo "Renamed:"
          head -n 100 renamed_files.txt
      - name: Process files
        id: process-files
        run: |
          # Function to join array elements with commas
          join_by_comma() {
            local IFS=","
            echo "$*"
          }

          # Function to process a chunk of files
          process_chunk() {
            local added=("$1")
            local modified=("$2")
            local deleted=("$3")
            local renamed=("$4")
            local temp_log=$(mktemp)
            local attempt=1
            local max_attempts=3

            while [ $attempt -le $max_attempts ]; do
              echo "Attempt $attempt of $max_attempts"
              echo "Processing chunk with:"
              echo "Added files: $(echo "$added" | tr ',' '\n' | wc -l)"
              echo "Modified files: $(echo "$modified" | tr ',' '\n' | wc -l)"
              echo "Deleted files: $(echo "$deleted" | tr ',' '\n' | wc -l)"
              echo "Renamed files: $(echo "$renamed" | tr ',' '\n' | wc -l)"

              if docker run --rm \
                -v ${{ github.workspace }}:/app \
                -e ALGOLIA_APP_ID=${{ secrets.ALGOLIA_APP_ID }} \
                -e ALGOLIA_API_KEY=${{ secrets.ALGOLIA_WRITE_KEY }} \
                vtexdocs/devportal-docsearch-action \
                --files ./configs/scraper_helpcenter_files.json \
                --added "$added" \
                --updated "$modified" \
                --removed "$deleted" \
                --renamed "$renamed" 2>&1 | tee -a "$temp_log"; then
                cat "$temp_log" >> "$GLOBAL_LOG"
                rm "$temp_log"
                return 0
              fi

              # Check for specific errors
              if grep -q "Maximum object size exceeded" "$temp_log"; then
                echo "Maximum object size exceeded error detected"
                echo "Error details:"
                grep -A 5 "Maximum object size exceeded" "$temp_log"
                cat "$temp_log" >> "$GLOBAL_LOG"
                rm "$temp_log"
                return 2
              fi

              if grep -q "Argument list too long" "$temp_log"; then
                echo "Argument list too long error detected"
                cat "$temp_log" >> "$GLOBAL_LOG"
                rm "$temp_log"
                return 2
              fi

              echo "Different error occurred, retrying..."
              echo "Error details:"
              cat "$temp_log"
              ((attempt++))
            done

            cat "$temp_log" >> "$GLOBAL_LOG"
            rm "$temp_log"
            return 1
          }

          # Function to process files in chunks
          process_files() {
            local file=$1
            local chunk_size=$2
            local type=$3
            local total=$(wc -l < "$file")
            local i=0

            while [ $i -lt $total ]; do
              local end=$((i + chunk_size))
              [ $end -gt $total ] && end=$total
              
              local chunk=$(sed -n "$((i+1)),${end}p" "$file" | tr '\n' ',' | sed 's/,$//')
              echo "Processing $type files: $((i+1)) to $end of $total"
              
              # Call process_chunk with the appropriate arguments
              local added_arg=""
              local modified_arg=""
              local deleted_arg=""
              local renamed_arg=""
              
              case "$type" in
                "added") added_arg="$chunk" ;;
                "modified") modified_arg="$chunk" ;;
                "deleted") deleted_arg="$chunk" ;;
                "renamed") renamed_arg="$chunk" ;;
              esac
              
              process_chunk "$added_arg" "$modified_arg" "$deleted_arg" "$renamed_arg"
              local result=$?
              
              if [ $result -eq 2 ]; then
                # If chunk was too large, reduce size and retry
                chunk_size=$((chunk_size / 2))
                echo "Reducing chunk size to $chunk_size"
                if [ $chunk_size -lt 5 ]; then
                  echo "Chunk size too small, aborting"
                  return 1
                fi
                continue
              elif [ $result -ne 0 ]; then
                return 1
              fi
              
              i=$end
            done
            return 0
          }

          # Function to extract URLs from error logs
          extract_error_urls() {
            local log_file=$1
            local error_urls=()
            while IFS= read -r line; do
              if [[ $line =~ "ERROR: Http Status:(404|500) on (https?://[^[:space:]]+)" ]]; then
                error_urls+=("${BASH_REMATCH[2]}")
              fi
            done < "$log_file"
            echo "${error_urls[@]}"
          }

          # Create a global log file
          GLOBAL_LOG=$(mktemp)

          # Process each type of files separately with a small initial chunk size
          CHUNK_SIZE=500
          
          if [ -s added_files.txt ]; then
            echo "Processing added files..."
            process_files added_files.txt $CHUNK_SIZE "added" || exit 1
          fi
          
          if [ -s modified_files.txt ]; then
            echo "Processing modified files..."
            process_files modified_files.txt $CHUNK_SIZE "modified" || exit 1
          fi
          
          if [ -s deleted_files.txt ]; then
            echo "Processing deleted files..."
            process_files deleted_files.txt $CHUNK_SIZE "deleted" || exit 1
          fi
          
          if [ -s renamed_files.txt ]; then
            echo "Processing renamed files..."
            process_files renamed_files.txt $CHUNK_SIZE "renamed" || exit 1
          fi

          # Extract and report error URLs
          error_urls=($(extract_error_urls "$GLOBAL_LOG"))
          if [ ${#error_urls[@]} -gt 0 ]; then
            echo "::warning::Found ${#error_urls[@]} URLs that returned 404 or 500 errors:"
            printf '%s\n' "${error_urls[@]}" | sort -u
          fi

          # Clean up
          rm "$GLOBAL_LOG"
          rm -f added_files.txt modified_files.txt deleted_files.txt renamed_files.txt
