# Workflow to manually trigger the scraping of all help center documentation files
name: "Help Center batch files scraper"

on:
  workflow_dispatch:
    inputs:
      directory:
        description: 'Directory to search for files'
        required: false
        type: string
        default: 'docs'
      batch_size:
        description: 'Number of files to process per batch'
        required: false
        type: number
        default: 300
      sleep_time:
        description: 'Time to sleep between batches'
        required: false
        type: number
        default: 300
      files:
        description: 'Comma-separated list of files to process'
        required: false
        type: string

jobs:
  find-files:
    if: inputs.files == ''
    runs-on: ubuntu-latest
    name: Find files and create batches
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Find all markdown files
        id: find-files
        run: |
          echo "Searching for files in ${{ inputs.directory }}"

          # Ensure we're only searching in the specified directory
          if [ ! -d "${{ inputs.directory }}" ]; then
            echo "Error: ${{ inputs.directory }} directory not found!"
            exit 1
          fi
          
          # Find all markdown files in the specified directory and its subdirectories
          FILES=$(find ${{ inputs.directory }} -type f \( -name "*.md" -o -name "*.mdx" \) | tr '\n' ',')
          
          if [ -z "$FILES" ]; then
            echo "Error: No markdown files found in ${{ inputs.directory }} directory!"
            exit 1
          fi
          
          echo "Found files:"
          echo "$FILES"
          echo "files<<EOF" >> "$GITHUB_OUTPUT"
          echo "$FILES" >> "$GITHUB_OUTPUT"
          echo "EOF" >> "$GITHUB_OUTPUT"

      - name: Create batches and trigger workflows
        run: |
          # Read all files into an array using comma as delimiter
          IFS=',' read -r -a all_files <<< "${{ steps.find-files.outputs.files }}"
          total_files=${#all_files[@]}
          batch_size=${{ inputs.batch_size }}
          
          echo "Total files to process: $total_files"
          
          # Process files in batches
          for ((i=0; i<total_files; i+=batch_size)); do
            # Get the current batch of files
            batch=()
            end=$((i + batch_size))
            if [ $end -gt $total_files ]; then
              end=$total_files
            fi
            
            for ((j=i; j<end; j++)); do
              batch+=("${all_files[j]}")
            done
            
            # Join the batch files with commas
            batch_files=$(IFS=','; echo "${batch[*]}")
            
            echo "Triggering workflow for batch $((i/batch_size + 1)) with ${#batch[@]} files"
            echo "Files in this batch: $batch_files"
            
            # Trigger a new workflow run with this batch
            gh workflow run docsearch-scraper-batch.yml -f files="$batch_files"
            
            # Add a small delay between triggers to avoid rate limits
            if [ $((i+batch_size)) -lt $total_files ]; then
              echo "Waiting ${{ inputs.sleep_time }} seconds before next batch..."
              sleep ${{ inputs.sleep_time }}
            fi
          done
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

  process-batch:
    if: inputs.files != ''
    runs-on: ubuntu-latest
    name: Process batch of files
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Helpcenter Scraper
        id: scraper
        uses: vtexdocs/devportal-docsearch-action@main
        with:
          algolia_application_id: '${{ secrets.ALGOLIA_APP_ID }}'
          algolia_api_key: ${{ secrets.ALGOLIA_WRITE_KEY }}
          files: './configs/scraper_helpcenter_files.json'
          added: ${{ inputs.files }}

      - name: Log 404 and 500 errors to CSV
        run: |
          # Define the CSV file path
          CSV_FILE="error_log.csv"
          
          # Ensure the CSV file exists and has a header
          if [ ! -f "$CSV_FILE" ]; then
            echo "URL,Status Code,Date,Time" > "$CSV_FILE"
          fi
          
          # Extract logs from the scraper step output
          echo "${{ steps.scraper.outputs.logs }}" > scraper_log.txt
          
          # Extract 404 and 500 errors and append them to the CSV file
          grep -Eo "Http Status:(404|500) on [^ ]*" scraper_log.txt | while read -r line; do
            STATUS_CODE=$(echo "$line" | grep -o "Http Status:[0-9]*" | cut -d':' -f2)
            URL=$(echo "$line" | grep -o "on [^ ]*" | cut -d' ' -f2)
            DATE=$(date +"%Y-%m-%d")
            TIME=$(date +"%H:%M:%S")
            echo "$URL,$STATUS_CODE,$DATE,$TIME" >> "$CSV_FILE"
          done
          
          # Clean up
          rm scraper_log.txt
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}