# Workflow to manually trigger the scraping of all help center documentation files
name: "Help Center batch files scraper"

on:
  workflow_dispatch:
    inputs:
      directory:
        description: 'Directory to search for files'
        required: false
        type: string
        default: 'docs'
      batch_size:
        description: 'Number of files to process per batch'
        required: false
        type: number
        default: 300
      sleep_time:
        description: 'Time to sleep between batches'
        required: false
        type: number
        default: 300
      files:
        description: 'Comma-separated list of files to process'
        required: false
        type: string

jobs:
  find-files:
    if: inputs.files == ''
    runs-on: ubuntu-latest
    name: Find files and create batches
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Find all markdown files
        id: find-files
        run: |
          echo "Searching for files in ${{ inputs.directory }}"

          # Ensure we're only searching in the specified directory
          if [ ! -d "${{ inputs.directory }}" ]; then
            echo "Error: ${{ inputs.directory }} directory not found!"
            exit 1
          fi
          
          # Find all markdown files in the specified directory and its subdirectories
          FILES=$(find ${{ inputs.directory }} -type f \( -name "*.md" -o -name "*.mdx" \) | tr '\n' ',')
          
          if [ -z "$FILES" ]; then
            echo "Error: No markdown files found in ${{ inputs.directory }} directory!"
            exit 1
          fi
          
          echo "Found files:"
          echo "$FILES"
          echo "files<<EOF" >> "$GITHUB_OUTPUT"
          echo "$FILES" >> "$GITHUB_OUTPUT"
          echo "EOF" >> "$GITHUB_OUTPUT"

      - name: Create batches and trigger workflows
        run: |
          # Read all files into an array using comma as delimiter
          IFS=',' read -r -a all_files <<< "${{ steps.find-files.outputs.files }}"
          total_files=${#all_files[@]}
          batch_size=${{ inputs.batch_size }}
          
          echo "Total files to process: $total_files"
          
          # Process files in batches
          for ((i=0; i<total_files; i+=batch_size)); do
            # Get the current batch of files
            batch=()
            end=$((i + batch_size))
            if [ $end -gt $total_files ]; then
              end=$total_files
            fi
            
            for ((j=i; j<end; j++)); do
              batch+=("${all_files[j]}")
            done
            
            # Join the batch files with commas
            batch_files=$(IFS=','; echo "${batch[*]}")
            
            echo "Triggering workflow for batch $((i/batch_size + 1)) with ${#batch[@]} files"
            echo "Files in this batch: $batch_files"
            
            # Trigger a new workflow run with this batch
            gh workflow run docsearch-scraper-batch.yml -f files="$batch_files"
            
            # Add a small delay between triggers to avoid rate limits
            if [ $((i+batch_size)) -lt $total_files ]; then
              echo "Waiting ${{ inputs.sleep_time }} seconds before next batch..."
              sleep ${{ inputs.sleep_time }}
            fi
          done
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

  process-batch:
    if: inputs.files != ''
    runs-on: ubuntu-latest
    name: Process batch of files
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Pull the latest error log file
        run: |
          # Ensure the repository is up-to-date
          git pull origin main
          
          # Define the CSV file path in the repository root
          CSV_FILE="${{ github.workspace }}/error_log.csv"
          
          # Ensure the CSV file exists and has a header if it doesn't exist
          if [ ! -f "$CSV_FILE" ]; then
            echo "URL,Status Code,Date,Time" > "$CSV_FILE"
          fi

      - name: Helpcenter Scraper
        id: scraper
        uses: vtexdocs/devportal-docsearch-action@main
        with:
          algolia_application_id: '${{ secrets.ALGOLIA_APP_ID }}'
          algolia_api_key: ${{ secrets.ALGOLIA_WRITE_KEY }}
          files: './configs/scraper_helpcenter_files.json'
          added: ${{ inputs.files }}
      
      - name: Upload Scraper Logs
        uses: actions/upload-artifact@v3
        with:
          name: scraper-logs
          path: scraper.log

      - name: Log 404 and 500 errors to CSV
        run: |
          # Define the CSV file path in the repository root
          CSV_FILE="${{ github.workspace }}/error_log.csv"
          
          # Extract logs from the scraper step output
          echo "${{ steps.scraper.outputs.errors }}" > scraper_errors.txt
          
          # Debug: Print the scraper log file content
          echo "Scraper log file content:"
          cat scraper_errors.txt
          
          # Extract 404 and 500 errors and append them to the CSV file
          grep -Eo "Http Status:(404|500) on [^ ]*" scraper_errors.txt | while read -r line; do
            STATUS_CODE=$(echo "$line" | grep -o "Http Status:[0-9]*" | cut -d':' -f2)
            URL=$(echo "$line" | grep -o "on [^ ]*" | cut -d' ' -f2)
            DATE=$(date +"%Y-%m-%d")
            TIME=$(date +"%H:%M:%S")
            echo "$URL,$STATUS_CODE,$DATE,$TIME" >> "$CSV_FILE"
          done
          
          # Debug: Print the updated CSV file content
          echo "Updated CSV file content:"
          cat "$CSV_FILE"
          
          # Clean up
          rm scraper_errors.txt
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Commit and push changes in error logs file
        run: |
          git config --global user.name "github-actions[bot]"
          git config --global user.email "github-actions[bot]@users.noreply.github.com"
          
          # Define the branch name
          BRANCH_NAME="search-indexing-error-logs"
          
          # Check if the branch exists remotely
          if git ls-remote --exit-code --heads origin "$BRANCH_NAME"; then
            echo "Branch $BRANCH_NAME exists. Fetching it."
            git fetch origin "$BRANCH_NAME"
            git checkout "$BRANCH_NAME"
          else
            echo "Branch $BRANCH_NAME does not exist. Creating it."
            git checkout -b "$BRANCH_NAME"
            git push -u origin "$BRANCH_NAME"  # Push the new branch to the remote repository
          fi
          
          # Pull the latest content from the branch
          git pull origin "$BRANCH_NAME" --rebase
          
          # Stage and commit changes
          git add error_log.csv
          git commit -m "Update error_log.csv with new search indexing errors" || echo "No changes to commit"
          
          # Push changes to the branch
          git push origin "$BRANCH_NAME"

      - name: Create Pull Request if it doesn't exist
        run: |
          # Check if a Pull Request already exists for the branch
          PR_EXISTS=$(gh pr list --head "$BRANCH_NAME" --state open --json number --jq '.[0].number')
          
          # Check if PR_EXISTS is empty
          if [ -z "$PR_EXISTS" ]; then
            echo "No open Pull Request found for branch $BRANCH_NAME. Creating one."
            gh pr create --title "Update search indexing error logs" --body "This PR updates the error_log.csv file with new search indexing errors." --head "$BRANCH_NAME" --base main
          else
            echo "Pull Request #$PR_EXISTS already exists for branch $BRANCH_NAME."
          fi
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}